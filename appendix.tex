\begin{appendices}
\section{Aitchison Geometry}
\textbf{Note for coauthors: These are taken from notes on Aitchison Geometry, more trimming and rewriting needs to happen to make them relevant + citing. Also, here $\mathcal{S}^N$ is the probability simplex $\Delta^{N}$ which is equivalent to the face of a unit simplex $\Delta^{N}$.\\}

A vector $\mathbf{x}=\left[x_1, x_2, \ldots, x_N\right]$ is a composition if $x_i \in \mathbb{R}_{+}^N$ and each $x_i$ represents relative information here. For any vector of $N$ real positive components, the closure of $\mathbf{x}$ is defined as
$$
\mathcal{C}(\mathbf{x})=\left[\frac{\kappa \cdot z_1}{\sum_{i=1}^N z_i}, \frac{\kappa \cdot z_2}{\sum_{i=1}^N z_i}, \cdots, \frac{\kappa \cdot z_N}{\sum_{i=1}^N z_i}\right] .
$$
The result is the same vector re-scaled so that the sum of its components is $\kappa$.\\


\textbf{General principles}

\begin{itemize}
	\item Scale Invariance: Two compositions $\mathbf{x}, \mathbf{y}$ are equivalent if there exists a positive scalar $\lambda \in \mathbb{R}^{+}$such that $\mathbf{x}=\lambda \cdot \mathbf{y}$ and, equivalently, $\mathcal{C}(\mathbf{x})=\mathcal{C}(\mathbf{y})$. For a scale invariant function $f(.)$ and a positive scalar $\lambda$, we notice $f(\lambda \mathbf{x})=$ $f(\mathbf{x})$.  Common choices of such functions are log-ratios of the parts in $\mathbf{x}$. The ratio $f(\mathbf{x})=x_1 / x_2=\left(\lambda \cdot x_1\right) /\left(\lambda \cdot x_2\right)$ is scale invariant.
	\item A function is permutation-invariant if it yields equivalent results when the ordering of the parts in the composition is changed. 
	\item Subcompositional coherence: Composition results obtained for a subset of parts of a composition (a subcomposition) should remain the same as in a composition

\end{itemize}
Perturbation of a composition $\mathbf{x} \in \mathcal{S}^N$ by a composition $\mathbf{y} \in$ $\mathcal{S}^N$
$$
\mathbf{x} \oplus \mathbf{y}=\mathcal{C}\left[x_1 y_1, x_2 y_2, \ldots, x_N y_N\right]
$$
Power transformation or powering of a composition $\mathbf{x} \in \mathcal{S}^N$ by a constant $\alpha \in \mathbb{R}$,
$$
\alpha \odot \mathbf{x}=\mathcal{C}\left[x_1^\alpha, x_2^\alpha, \ldots, x_N^\alpha\right] .
$$
The simplex,$\left(\mathcal{S}^N, \oplus, \odot\right)$, with perturbation and powering, is a vector space. This means the following properties hold, making them analogous to translation and scalar multiplication:
$\left(\mathcal{S}^N, \oplus\right)$ has a commutative group structure
Powering satisfies the properties of an external product.

Inner product of $\mathbf{x}, \mathbf{y} \in \mathcal{S}^N$,
$$
\langle\mathbf{x}, \mathbf{y}\rangle_a=\frac{1}{2 D} \sum_{i=1}^N \sum_{j=1}^N \ln \frac{x_i}{x_j} \ln \frac{y_i}{y_j}
$$

Norm of $\mathbf{x} \in \mathcal{S}^N$
$$
\|\mathbf{x}\|_a=\sqrt{\frac{1}{2 D} \sum_{i=1}^N \sum_{j=1}^N\left(\ln \frac{x_i}{x_j}\right)^2}
$$

Distance between $\mathbf{x}$ and $\mathbf{y} \in \mathcal{S}^N$
$$
d_a(\mathbf{x}, \mathbf{y})=\|\mathbf{x} \ominus \mathbf{x}\|_a=\sqrt{\frac{1}{2 D} \sum_{i=1}^N \sum_{j=1}^N\left(\ln \frac{x_i}{x_j}-\ln \frac{y_i}{y_j}\right)^2}
$$
The algebraic-geometric structure of $\mathcal{S}^N$ satisfies standard properties, like compatibility of the distance with perturbation and powering, i.e.
$$
\mathrm{N}_a(\mathbf{p} \oplus \mathbf{x}, \mathbf{p} \oplus \mathbf{y})=\mathrm{N}_a(\mathbf{x}, \mathbf{y}), \quad \mathrm{N}_a(\alpha \odot \mathbf{x}, \alpha \odot \mathbf{y})=|\alpha| \mathrm{N}_a(\mathbf{x}, \mathbf{y})
$$
for any $\mathbf{x}, \mathbf{y}, \mathbf{p} \in \mathcal{S}^N$ and $\alpha \in \mathbb{R}$. Other typical properties of metric spaces are valid for $\mathcal{S}^N$. 

\subsection{Log ratios}
Given a composition $\mathbf{x}=\left[\begin{array}{llll}x_1 & x_2 & \cdots & x_N\end{array}\right]$, a general LR is defined as
$$
\begin{aligned}
\operatorname{LR}(j, k)= & \log \left(x_j / x_k\right), \text{where } j \neq k \text{and } j, k \in\{1, \ldots, N\}\\
=&  \log \left(x_j\right)-\log \left(x_k\right)
\end{aligned}
$$
\subsubsection{ALR}
$\begin{aligned} \operatorname{ALR}(\mathbf{x}) &=\left[\ln \left(\frac{x_1}{x_N}\right), \ldots, \ln \left(\frac{x_{N-1}}{x_N}\right)\right]^T \\ \operatorname{ALR}^{-1}(\mathbf{y}) &=\mathcal{C}\left(\left[\exp \left(y_1\right), \ldots, \exp \left(y_{N-1}\right), 1\right]^T\right) \end{aligned}$

$y_i$ varies monotonically with $x_i$, ALR is symmetric in the first $N$-1 dimensions, but not in the $N_{th}$. The simplex obtained from ALR is a linear combination of vectors in $\mathbb{R}^{N-1}$ with weights $\ln \left(x_i\right)$. The first $(N-1)$ vectors are the standard basis vectors of $\mathbb{R}^{N-1},\left\{\mathbf{e}_1, \cdots, \mathbf{e}_{N-1}\right\}$, whereas the $N^{t h}$ vector is $-1$. Since $1=\left\|\mathbf{e}_i\right\|<\|-\mathbf{1}\|=\sqrt{N-1}$, for the binary case $N=2$ the ALR transform is symmetric between the two labels, but for $N>2$, changes in the last label's probability are magnified by $\sqrt{N-1}$.  

\subsubsection{ILR}
$$
\operatorname{ILR}(\mathbf{p})=U^T \ln (\mathbf{p})
$$
$$
\mathrm{ILR}^{-1}(\mathbf{v})=\mathcal{C}(\exp (U \mathbf{v}))
$$
where $\mathcal{C}(\cdot)$ renormalizes and $U=\left[\mathbf{u}_1, \ldots, \mathbf{u}_{L-1}\right]$ is an orthonormal basis for the hyperplane of $\mathbb{R}^L$ orthogonal to 1 . In ILR, the vectors equidistant from each other and from the origin (forming the vertices of a generalized tetrahedron), making ILR symmetric in all N dimensions. ILR maps the Aitchison inner product to the Euclidean inner product (unlike ALR which maps to a skewed euclidean inner product)
\\

{{insert figure here}}


\section{Jacobian Determinants}
\subsection{Stickbreaking Transform}
The Jacobian matrix for $f^{-1}$ is a lower-triangular diagonal matrix, so for the change of variables we evaluate $\mathbf{J}_{i, i}$ where $i \in 1:N-1$.
\begin{align*}
\mathbf{J}_{i, i} &= \frac{\partial x_i}{\partial y_i}
=
\frac{\partial x_i}{\partial z_i} \,
\frac{\partial z_i}{\partial y_i}\\
\mathbf{J}_{i, i} &= \left(
  1 - \sum_{k' = 1}^{k-1} x_{k'}
   \right) z_k (1 - z_k),\\
   \abs{\textbf{J}} &= \prod_{i=1}^{N-1} \textbf{J}_{i,i}
\end{align*}

The change of variables adjustment $p_Y(y) = p_X(f^{-1}(y))\,
\prod_{i=1}^{N-1}z_i\,(1 - z_i)\left(1 - \sum_{i'=1}^{i-1} x_{i'}\right).$

\subsection{Additive Log ratio transform}
To calculate the determinant of the Jacobian of the inverse transform,
we start by noting that $s = \textrm{exp} \circ \textrm{norm}$, where
$\textrm{exp}$ is the elementwise exponential function and
\textrm{norm} is defined by
\[
  \textrm{norm}(z) = \frac{z}{\textrm{sum}(z) + 1}.
\]
As such, the resulting Jacobian determinant is the product of the
Jacobian determinants of the component functions,
\[
  \absdet{J_s(y)}
  = \absdet{J_{\textrm{exp}}(y)} \absdet{J_{\textrm{norm}}(z)},
\]
where $z = \textrm{exp}(y)$.  The Jacobian for the exponential
function is diagonal, so the determinant is the product of the
diagonal of the Jacobian, which for $y \in \mathbb{R}^{N-1}$ is
\[
  \absdet{J_{\textrm{exp}}(y)} = \textrm{prod}(\exp(y)).
\]
As above, let $z = \exp(y) \in (0, \inf)^{N-1}$.  We can differentiate
$\textrm{norm}$ to derive the Jacobian,
\[
  J_{\textrm{norm}}
  = \frac{1}{1 + \textrm{sum}(z)} \mathbb{I}_{N-1}
  - \left(\frac{1}{(1 + \textrm{sum}(z))^2} \beta \right)
  \textrm{vector}_{N-1}(1)^{\top},
\]
where $\mathbb{I}_{N-1}$ is the $(N - 1) \times (N - 1)$ unit matrix and
$\textrm{vector}_{N-1}(1)$ is the $N - 1$-vector with values 1.  Using
the matrix determinant lemma,\footnote{The matrix determinant lemma
  is \[\textrm{det}(A + u v^{\top}) = (1 + v^{\top} A^{-1} u)
    \textrm{det}(A).\]}
we have
\begin{eqnarray*}
  \textrm{absdet}(J_{\textrm{norm}}(z))
  & = &
  \left(
    1
    + \textrm{vector}_{N-1}(1)^{\top}
    \left(\frac{1}{1 + \textrm{sum}(z)} \mathbb{I} \right)^{-1}
    \frac{-z}{(1 + \textrm{sum}(z))^2}
    \right)
    \ \textrm{det}\left(\frac{1}{1 + \textrm{sum}(z)} \mathbb{I}
        \right)
  \\[6pt]
  & = &
  \left(
    1 
    + \textrm{sum}\left( \frac{-(1 + \textrm{sum}(z)) z}{(1 +
        \textrm{sum}(z))^2} \right)
  \right)
        \ \left( \frac{1}{1 + \textrm{sum}(z)} \right)^{N-1}
  \\[6pt]
  & = &
        \left(1 + \textrm{sum}\left(\frac{-z}{1 + \textrm{sum}(z)} \right)\right)        
        \ \left( \frac{1}{1 + \textrm{sum}(z)} \right)^{N-1}
  \\[6pt]
  & = & \left( 1 - \textrm{sum}(\textrm{norm}(z)) \right) 
        \ \left( \frac{1}{1 + \textrm{sum}(z)} \right)^{N-1}
  \\[6pt]
  & = & \left( \frac{1}{1 + \textrm{sum}(z)} \right)^N.
\end{eqnarray*}
Thus the entire absolute determinant of the Jacobian is defined by the
product, 
\[
  \absdet{J_s(y)}
  \ = \
  \textrm{prod}(\exp(y))
  \, \left( \frac{1}{1 + \textrm{sum}(\exp(y))} \right)^N.
\]
and our final expression for densities for unconstrained $y \in
\mathbb{R}^{N-1}$ is
\[
  p_Y(y)
  = p_X(\textrm{alr}^{-1}(y))
  \, \textrm{prod}(\exp(y))
  \left( \frac{1}{1 + \textrm{sum}(\exp(y))} \right)^N
\]

\subsection{Hyperspherical simplex transforms}

\subsubsection{Hyperspherical coordinate transform}

We use the following definition of hyperspherical coordinates.
For $\phi_i \in (0, \pi/2]$ and $u \in \mathbb{S}^N_{>0}$, the positive orthant of the $N$-sphere,
\[
  u_i = \left(\prod_{k=1}^{i-1} \sin(\phi_k)\right) \begin{cases}
    1 & \text{ if } i = N + 1\\
    \cos(\phi_i) & \text{otherwise}
  \end{cases}.
\]

Let $z_i = \sin^2(\phi_i)$ and $x_i = u_i^2$, so that $x \in \Delta^N$.
Then
\[
  x_i = \left(\prod_{k=1}^{i-1} z_k\right) \left(\begin{cases}
    1 & \text{ if } i = N + 1\\
    (1 - z_i) & \text{otherwise}
  \end{cases}\right) = (1 - z_i (1 - \delta_{i,N+1})) \prod_{k=1}^{i-1} z_k.
\]

\subsubsection{Hyperspherical coordinates inverse transform}

To derive the inverse transform, note that
\[
    \begin{aligned}
        x_{N+1} &= \prod_{k=1}^{N} z_k\\
        x_N + x_{N+1} &= (1 - z_N)\left(\prod_{k=1}^{N-1} z_k\right) + z_N \left( \prod_{k=1}^{N-1} z_k \right) = \prod_{k=1}^{N-1} z_k = \frac{x_N}{1 - z_N}\\
        x_{N-1} + x_{N} + x_{N+1} &= (1-z_{N-1}) \left(\prod_{k=1}^{N-2} z_k\right) + z_{N-1} \left(\prod_{k=1}^{N-2} z_k\right) = \prod_{k=1}^{N-2} z_k = \frac{x_{N-1}}{1 - z_{N-1}}\\
        &\vdots \\
        \sum_{k=i}^{N+1} x_{k} &= \prod_{k=1}^{i - 1} z_k = \frac{x_i}{1 - z_i}.
    \end{aligned}
\]

From the final expression, we can define the inverse transform:
\[
z_i = 1 - \frac{x_i}{\sum_{k=i}^{N+1} x_k}
\]

\subsubsection{Jacobian of hyperspherical coordinates transform}

To derive the Jacobian, we note that $x_i$ depends only on $z_k$ for $k \le i$.
As a result, the Jacobian of the map from $z$ to $x_-$ is lower triangular, and its determinant depends only on its diagonal, which consists of the terms
$$J_{ii} = -\frac{\partial x_i}{\partial z_i} = \prod_{k=1}^{i-1} z_k,$$
so
$$
|J| = \prod_{i=1}^N |J_{ii}|
= \prod_{i=1}^N \prod_{k=1}^{i-1} z_k
= \prod_{k=1}^{N-1} \prod_{i=k+1}^N z_k
= \prod_{i=1}^{N-1} z_i^{N-i} = \prod_{i=1}^N z_i^{N-i}
$$

Due to the form of the Jacobian correction, if $x$ is uniformly distributed on the unit simplex, then $z_i$ and $z_j$ are uncorrelated for all $i \ne j$.

\subsubsection{Hyperspherical angular transform}

The map from $y$ to $z$ is elementwise, so its Jacobian is diagonal.
Each element is
$$\sin^{-1}(\sqrt{z_i}) = \phi_i = \frac{\pi}{2} \operatorname{logit}^{-1} (y_i).$$
Differentiating all sides with respect to $y_i$, we find
$$
\begin{aligned}
\frac{1}{\sqrt{1 - z_i^2}} \frac{1}{2 \sqrt{z_i}} \frac{\partial z_i}{\partial y_i} &= \frac{\pi}{2} \operatorname{logit}^{-1} (y_i) (1 - \operatorname{logit}^{-1} (y_i)) = \phi_i \left(1 - \frac{2}{\pi} \phi_i\right)\\
\frac{\partial z_i}{\partial y_i} &= 2 \phi_i \left(1 - \frac{2}{\pi} \phi_i\right) \sin(\phi_i) \cos(\phi_i).
\end{aligned}
$$

As a result, the combined transform $f: y \mapsto x$ is
\[
  x_i = \left(1 - \cos^2\left(\frac{\pi}{2} \operatorname{logit}^{-1} (y_i)\right) (1 - \delta_{i,N+1})\right)\left(\prod_{k=1}^{i-1} \sin^2\left(\frac{\pi}{2} \operatorname{logit}^{-1} (y_k)\right)\right),
\]
and its inverse is
\[
z_i = \operatorname{logit}\left(\frac{2}{\pi}\sin^{-1}\left(\sqrt{1 - \frac{x_i}{\sum_{k=i}^{N+1} x_k}}\right)\right).
\]

The Jacobian determinant is
$$
\begin{aligned}
    |J_f| &= \prod_{i=1}^N z_i^{N-i} 2 \phi_i \left(1 - \frac{2}{\pi} \phi_i\right) \sin(\phi_i) \cos(\phi_i)\\
          &= 2^N \prod_{i=1}^N \phi_i \left(1 - \frac{2}{\pi} \phi_i\right) \sin^{2(N-i)+1}(\phi_i) \cos(\phi_i).
\end{aligned}
$$

\subsubsection{Hyperspherical logit transform}

For $z_i \in (0, 1]$, $z_i^k \in (0, 1]$ is bijective for $k \ne 0$.
Let $g(w)$ be the density function for a distribution on $\mathbb{R}$ and $G(w)$ be its cumulative distribution function.
By the definition of the CDF, $\frac{\mathrm{d}}{\mathrm{d}w} G(w) = g(w)$.
Let $z_i^{N-i+1} = G(y_i)$.

The combined transform $f: y \mapsto x$ is then
\[
  x_i = \left(1-G(y_i)^{1/(N-i+1)}(1 - \delta_{i,N+1})\right)\left(\prod_{k=1}^{i-1} G(y_k)^{1/(N-k+1)}\right),
\]
and its inverse is 
\[
y_i = G^{-1}\left(\left(1 - \frac{x_i}{\sum_{k=i}^{N+1} x_k}\right)^{N-i+1}\right).
\]

Then
$$
\begin{aligned}
(N-i+1) z_i^{N-i} \frac{\partial z_i}{\partial y_i} &= g(y_i)\\
\frac{\partial z_i}{\partial y_i} &= \frac{g(y_i)}{(N-i+1) z_i}.
\end{aligned}
$$

As a result,
$$
\begin{aligned}
|J_f| &= \prod_{i=1}^{N} z_i^{N-i} \frac{g(y_i)}{(N-i+1) z_i^{N-i}}\\ 
&= \left(\prod_{i=1}^N \frac{1}{N-i+1} \right) \prod_{i=1}^{N} g(y_i)\\
&= \frac{1}{N!} \prod_{i=1}^{N} g(y_i).
\end{aligned}
$$

This particular transform allows us to transform the uniform distribution on the simplex to any distribution on $\mathbb{R}^N$ whose coordinates are IID distributed according to $g$.

If we select $G$ to be the logistic function, then $g(w) = \operatorname{logit}^{-1}(w) (1 - \operatorname{logit}^{-1}(w))$ is the density of the logistic distribution with mean $0$ and scale term $1$, and $y_i$ is logistically distributed.
This choice of $G$ defines the hyperspherical logit transform.

\subsubsection{Hyperspherical probit transform}

Similarly, to define the hyperspherical probit transform, we select $G$ to be the inverse probit function.
Then $g$ is the density function of the standard normal distribution.

\subsubsection{Transforming the Dirichlet distribution}

Recall that the Dirichlet distribution has density $p_X(x)$ equal to
\[
p_X(x) \propto \prod_{i=1}^{N+1} x_i^{\alpha_i - 1}.
\]

Note that
\[
\begin{aligned}
\prod_{i=1}^{N+1} x_i^{\alpha_i - 1} &= \prod_{i=1}^{N+1} \left(\left(1-G(y_i)^{1/(N-i+1)}(1 - \delta_{i,N+1})\right)\left(\prod_{k=1}^{i-1} G(y_k)^{1/(N-k+1)}\right)\right)^{\alpha_i - 1}\\
&= \left(\prod_{i=1}^{N} \left(1-G(y_i)^{1/(N-i+1)}\right)^{\alpha_i - 1}\right) \prod_{i=1}^{N+1}\prod_{k=1}^{i-1} G(y_k)^{(\alpha_i - 1)/(N-k+1)}\\
&= \prod_{i=1}^{N} \left(1-G(y_i)^{1/(N-i+1)}\right)^{\alpha_i - 1} G(y_i)^{-1+\sum_{k=i+1}^{N+1}\alpha_k/(N-i+1)}\\
\end{aligned}
\]
The transformed density on $y$ is then
\[
p_Y(y) \propto \prod_{i=1}^{N} \left(1-G(y_i)^{1/(N-i+1)}\right)^{\alpha_i - 1} G(y_i)^{-1+\sum_{k=i+1}^{N+1}\alpha_k/(N-i+1)} g(y_i) = \prod_{i=1}^{N} h(y_i, \alpha_{i:N+1}),
\]
where $h$ is a density function.
Therefore, regardless of the choice of $g$ or $\alpha$, the individual coordinates of the transformed distribution are independently distributed.


\end{appendices}