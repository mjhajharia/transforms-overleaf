\documentclass[11pt]{article}
    
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{cite}
\usepackage[letterpaper, total={6.5in, 9in}]{geometry}
\usepackage{mathpazo}
\usepackage{sourcecodepro}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
    \hypersetup{
        colorlinks=true,
        linkcolor=MidnightBlue,
        anchorcolor=MidnightBlue,
        citecolor=MidnightBlue,
        filecolor=MidnightBlue,
        urlcolor=MidnightBlue,
        pdftitle={Efficient Unconstraining Transforms for HMC},
        pdfauthor={Meenal Jhajaria et al.},
        pdfpagemode=FullScreen,
    }
\usepackage[title]{appendix}

\newcommand{\setcomp}[2]{\left\{ #1 \ \Big|\ #2 \right\}}
\newcommand{\rngto}[1]{1{:}#1}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\absdet}[1]{\abs{#1}}
\newcommand{\dv}[1]{\mathrm{d}{#1}}
\newcommand{\Exp}{\mathrm{Exp}}
\newcommand{\vect}{\mathrm{vec}}
\newcommand{\vectu}{\mathrm{vecu}}

\begin{document}


\title{Efficient Unconstraining
  Parameter Transforms for Hamiltonian Monte Carlo}
\author{Meenal Jhajharia \\ \small Flatiron Institute 
\and Seth Axen \\ \small University of T\"ubingen 
\and Adam Haber \\ \small Weizmann Institute 
\and Sean Pinkney \\ \small Omnicom Media Group
\and Bob Carpenter \\ \small Flatiron Institute}
\date{DRAFT: \today}
\maketitle


\begin{abstract}
  \noindent
  This paper evaluates the statistical and computational
  efficiency of unconstraining parameter transforms for Hamiltonian
  Monte Carlo sampling.
\end{abstract}

\section{Introduction}

In statistical computing, we often need to compute high-dimensional
integrals over densities $\pi(x)$ (e.g., Bayesian estimation or
prediction, $p$-value calculations, etc.).  The only black-box
techniques that work for general high-dimensional integrals are
Markov chain Monte Carlo (MCMC) methods.  The most effective MCMC
method in high dimensions is Hamiltonian Monte Carlo (HMC) \cite{neal2011mcmc}.
HMC works by simulating the Hamiltonian dynamics  a fictitious particle
representing the value being sampled coupled with a momentum term.

Although it is possible to write HMC samplers that work for simply
constrained values such as upper- and/or lower-bounds
\cite{neal2011mcmc} or unit vectors \cite{byrne2013geodesic}, it is
much more challenging to do the same for complex constraints such as
simplexes or positive definite matrices or for densities involving
multiple constrained values.  Instead, it is far more common to map
the constrained values to unconstrained values before sampling
\cite{JSSv076i01, radul2021base, fjelde2020bijectors}.  

Our goal in this paper is to evaluate a standard and less well known transforms for constrained data types in regular use in statistical modeling.  We will evaluate transforms in the head, body, and tail of distributions in terms of computational efficiency of both the transform, its Jacobian determinant and associated gradients, the geometry induced in the unconstrained space (e.g., log convexity and conditioning), as well as Hamiltonian Monte Carlo sampling efficiency. 

We consider transforms for several constrained data types, including scalars with lower- and/or upper-bounds, vectors with simplex, sum-to-zero or unit length constraints, and matrices (or their Cholesky factors) constrained to be positive definite (e.g., covariance matrices), positive definite with unit diagonal (e.g., correlation matrices), or orthonormal.  

\section{Constraining transforms}

In this paper, we restrict our attention to a general class of surjective constraining transforms $f : \mathcal{Y} \rightarrow \mathcal{X}$ from an unconstrained space $\mathcal{Y}$ onto a constrained space $\mathcal{X}$.  For all of our transforms, we have $\mathcal{Y} = \mathbb{R}^M$ and $\mathcal{X} \subset \mathbb{R}^N$.  Along with each transform, we have an additional (perhaps improper) density function $h(y)$ defined over $y \in \mathcal{Y}$ such that if $y \sim h(\cdot)$, then $f(y) \sim \textrm{uniform}(\mathcal{X}).$  

The above definition is informal in that it's not possible to sample over an improper density.  Instead, we can characterize our end-to-end goal, which is that if we have a proper density $p_X(x)$ defined for $x \in \mathcal{X}$, then we want the induced distribution on $y$, namely
$$
p_Y(y) = h(y) \, p_X(f(y)),
$$
to be proper and be such that if $y \sim p_Y(\cdot)$, then $f(y) \sim p_X(\cdot)$.

\subsection{Bijective constraining transforms}

The traditional way to define constraining transforms is through the usual change-of-variables formula.  Suppose we have a distribution $p_X(x)$ on constrained values $x \in \mathcal{X}$ and we have a smooth and bijective unconstraining transform $g : \mathcal{X} \rightarrow \mathcal{Y}$.  We then define our constraining transform $f : \mathcal{Y} \rightarrow \mathcal{X}$ as the inverse of $g$, $f = g^{-1}$.  The appropriate density on $\mathcal{Y}$ is then defined as the absolute value of the determinant of the constraining transform
\[
h(y) = \absdet{J_f(y)}.
\]
That way, for any given $p_X(x)$, we have
\[
p_Y(y) = p_X(f(y)) \, \absdet{J_f(y)},
\]
where the Jacobian of the inverse transform is defined by
\[
  J_{f^{-1}}(y) \ = \ \nabla_y \, f^{-1}(y) \ = \ \frac{\partial}{\partial y} \, f^{-1}(y).
\]
Because we have applied a standard change of variables adjustment, we are guaranteed that if $y \sim p_Y(\cdot)$, then $f(y) \sim p_X(\cdot)$.

\subsection{Surjective constraining transforms}

Not all of our transforms are simple bijective changes of variables as described in the previous subsection.  In some cases, we will take the dimensionality of the unconstrained space $\mathcal{Y}$ to be higher than that of the constrained space $\mathcal{X}$ and thus need additional constraints to ensure the correct distributions on $\mathcal{X}$.  In all of these cases, we will have a $p_Y(y)$ defined as a product of the change-of-variables adjustment for the transform coupled with a standard normal distribution over $\mathcal{Y}$,
\[
  h(y) = \textrm{normal}(y \mid 0, \textrm{I}) \ \absdet{J_f(y)}.
\]  
In each case, we will have to ensure that $p_Y(y) = h(y) \, p_X(f(y))$ is such that if $y \sim p_Y(\cdot)$, then $f(y) \sim p_X(\cdot)$.




\section{Vector transforms}

In this section, we consider transforms from unconstrained space to constrained vectors, including unit-simplex, sum-to-zero, and unit-length constraints.

\subsection{Unit simplex}

A unit $N$-simplex is an $N + 1$-dimensional vector of non-negative
values that sums to one.  Simplexes are useful for representations of multinomial probabilities
(e.g., probabilities of categories in a classification problem). The set of unit $N$-simplexes is conventionally denoted
\[
  \Delta^N = \setcomp{x \in \mathbb{R}_{\ge 0}^{N + 1}}{ \sum_{i=1}^{N} x_i = 1}
\]
Geometrically, an $N$-simplex is the convex closure of $N+1$ points
that are 1 in one coordinate and 0 elsewhere.  For example, the
3-simplex is the complex closure of
$\begin{bmatrix}1 & 0 & 0 \end{bmatrix},
\begin{bmatrix} 0 & 1 & 0 \end{bmatrix}$,
and $\begin{bmatrix} 0 & 0 & 1 \end{bmatrix}$. As such, there are only $N$ degrees of
freedom, because if $x$ is an $N$-simplex, then
\[
  x_N = 1 - (x_1 + x_2 + \cdots + x_{N-1}).
\]
We will use $\Delta^N_-$ to denote $N$ elements of a simplex, this is sufficient to uniquely determine $\Delta^N$.
\subsection{Stick-Breaking Transform}

The stick-breaking transform carries forward the intuition in the stick-breaking construction for Dirichlet \cite{sethuraman1994constructive}. It is a process of recursively breaking a piece $x_i$ from a stick of unit length, where the leftover stick in the $i^{th}$ iteration is $ 1 - \sum_{1}^{i}x$. Let $y = f(x)$, then we define the stick-breaking mapping $ f \colon \Delta^{N-1} \to  R^{N-1}$, for $1 \leq i \leq N$ as:	
\[
y_i
= \mathrm{logit}(z_i) - \mbox{log}\left(\frac{1}{N-i}
   \right) 
   \]
  
for break proportion 
\[ 
 z_i = \frac{x_i}{1 - \sum_{i' = 1}^{i-1} x_{i'}}.
\]

The inverse transform $ f^{-1} \colon R^{N-1} \to \Delta^{N-1}$ is defined as:

\[
x_i = \left( 1 - \sum_{i'=1}^{i-1} x_{i'} \right)\\
\]

for break proportion \[z_i = \mathrm{logit}^{-1} \left( y_i  + \log \left( \frac{1}{N - i}
                                            \right)\right)
\]
                                            
The determinant of the Jacobian
\[
   \abs{\mathbf{J}} = \prod_{i=1}^{N-1}z_i\,(1 - z_i)\left(1 - \sum_{i'=1}^{i-1} x_{i'}\right)
\]
\subsection{Additive log ratio transform}
The unconstraining transform for the identified softmax is known as
the additive log ratio (ALR) transform
\cite{aitchison1982statistical}, which is a bijection
$\textrm{alr}:\Delta^{N-1} \rightarrow \mathbb{R}^{N-1}$ defined for
$x \in \Delta^{N-1}$ by
\[
  \textrm{alr}(x)
  = \begin{bmatrix}\displaystyle
    \log \frac{x_1}{x_N} \cdots \log \frac{x_{N-1}}{x_N}
  \end{bmatrix}
\]
The inverse additive log ratio transform maps values in
$\mathbb{R}^{N-1}$ to $\Delta^{N-1}$ defined for $y \in
\mathbb{R}^{N-1}$ by
\[
  \textrm{alr}^{-1}(y)
  = \textrm{softmax}(\begin{bmatrix} y &  0 \end{bmatrix}),
\]
where for $u \in \mathbb{R}^N$,

\[
  \textrm{softmax}(u) = \frac{exp(u)}{\sum exp(u)}
\]

\[
\textrm{Here, } \abs{J} = \prod \exp(y)
  \, \left( \frac{1}{1 + \sum(\exp(y))} \right)^N
\]
\subsection{Augmented-Softmax Transform}
%TODO:  Needs to be written like other transforms -> transform, inverse, |J|
We define the transformation
$\phi: \mathbb{R}^n \to \Delta^{N} \times \mathbb{R}_{>0}: y \mapsto
(x_-, r)$, where  $\Delta_{-}^N$ and $x_{-}$  denote $N$ elements of the simplex and $x$, respectively. Here $r = \sum_{i=1}^n \textrm{exp}(y_i)$,
$x_i = \frac{1}{r} \textrm{exp}(y_i)$ for $i \in [1, N-1]$ and $\delta_{ij}$ is the Kronecker delta function. If $\mathrm{diag}(x)_{i, j} = \delta_{ij} x_i$ and $\boldsymbol{1}_n$ is the $N$-vector of ones.\[
  J = (I_{N-1} - x_- \boldsymbol{1}_{N-1}^\top) \operatorname{diag}(x_-),
\] 

Using Sylvester's determinant theorem,
$|I_{N-1} - x_- \boldsymbol{1}_{N-1}^\top| = 1 -
\boldsymbol{1}_{N-1}^\top x_- = 1 - \sum_{i=1}^{N-1} x_i = x_N$, so
$$ |J| = x_n \prod_{i=1}^{N-1} x_i = \prod_{i=1}^{N} x_i = \exp\left(\sum_{i=1}^{N-1} y_i\right) \left(1 + \sum_{i=1}^{N-1} e^{y_i}\right)^{-N}$$

%TODO: better captions
\section{Results}

\begin{table}[!ht]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        ~ & stan & softmax-augmented & softmax & stickbreaking \\ \hline
        $\alpha = 0.1, \, N = 10$ & 6.908s & 11.712s & 10.548s & 13.386s \\ 
        $\alpha = 0.1, \, N = 100$ & 74.274s & 33.757s & 61.826s & 159.016s \\ 
        $\alpha = 0.1, \, N = 1000$ & 1108.48s & 307.463s & 657.791s & 11773.4s \\ 
        $\alpha = 1, \, N = 10$ & 4.072s & 6.328s & 5.337s & 5.67s \\ 
        $\alpha = 1, \, N = 100$ & 20.805s & 27.151s & 25.94s & 51.45s \\ 
        $\alpha = 1, \, N = 1000$ & 306.942s & 171.824s & 184.126 & 3023.24 \\ 
        $\alpha = 10, \, N = 10$ & 4.123s & 6.375s & 5.563s & 5.606s \\ 
        $\alpha = 10, \, N = 100$ & 23.6s & 29.091s & 30.747s & 48.834s \\ 
        $\alpha = 10, \, N = 1000$ & 205s.627 & 161.11s & 150.751s & 1866.83s \\ \hline
    \end{tabular}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.2\textwidth]{figures/simplex/ess_density.png}
    \caption{Probability Density plot of Effective Sample Size/Total LeapFrog Steps. Effective sample size is evaluated at 100 runs, each point is the value of ESS/Total Leapfrog steps taken by the sampler in one run.}
    \label{fig:ess_density}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.2\textwidth]{figures/simplex/ess_cdf.png}
    \caption{Cumulative Density plot of Effective Sample Size/Total LeapFrog Steps. Effective sample size is evaluated at 100 runs, each point is the value of ESS/Total Leapfrog steps taken by the sampler in one run.}
    \label{fig:ess_cdf}
\end{figure}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1.2\textwidth]{figures/simplex/rmse.png}
    \caption{Root Mean Squared Error vs Cumulative Leapfrog Steps. At every iteration in a run, cumulative mean of samples is used to calculate RMSE, which is plotted against the cumulative leapfrog steps till that iteration}
    \label{fig:rmse}
\end{figure}

\subsubsection*{Acknowledgements}

We would like to thank \url{matrixcalculus.org} for providing an
easy-to-use symbolic matrix derivative calculator.



\bibliography{all}{}
\bibliographystyle{plain}

\include{appendix}
\end{document}
